{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Dimension of the smallest images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from skimage.io import imread_collection\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import statistics\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9220\n"
     ]
    }
   ],
   "source": [
    "#train directory and loading in images\n",
    "train_dir = \"C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/train/*.jpg\"\n",
    "train = imread_collection(train_dir)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2305\n"
     ]
    }
   ],
   "source": [
    "#test directory and loading in images\n",
    "test_dir = \"C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/test/*.jpg\"\n",
    "test = imread_collection(test_dir)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "#Get the median dimension of the image in the training and testing directory\n",
    "lst=[]\n",
    "for i in range(len(train)):\n",
    "    lst.append(train[i].shape[0])\n",
    "for i in range(len(test)):\n",
    "    lst.append(test[i].shape[0])\n",
    "size=statistics.median(lst)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class_names = ['female','male']\n",
    "IMG_SIZE = size # pick the median dimension for resizing of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the training data csv for the labels\n",
    "df=pd.read_csv(\"C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/Training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that reads the data from a training data set and tag it with the correct label based on the csv\n",
    "IMG_DIMS = (size, size)\n",
    "def get_data(path):\n",
    "    data = []\n",
    "    files = glob.glob(path+\"/*\") # get files in each folder(class)\n",
    "    for f in files:\n",
    "        img = cv2.imread(f) #read the image\n",
    "        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE)) #resize the image\n",
    "        if df.loc[df['filename'] == os.path.basename(f), 'label'].item()==\"female\":\n",
    "            x=[0,1] #female is 1\n",
    "        else:\n",
    "            x=[1,0] #male is 0\n",
    "        data.append([np.array(img)/255,np.array(x)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the training set data prepared\n",
    "training_data = get_data('C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the shape of the training data array\n",
    "training_data[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG 16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 56, 56, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 56, 56, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 7, 7, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#define INPUT shape\n",
    "INPUT_SHAPE=(size, size, 3)\n",
    "\n",
    "#get VGG 16 model\n",
    "vgg_layers = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False,\n",
    "input_shape=INPUT_SHAPE)\n",
    "\n",
    "vgg_layers.summary() #view summary of the vgg layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x00000262BA8C52C8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C459E608> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C45DC588> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000262C4803788> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C480F088> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C4811908> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000262C481C608> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C4816A08> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48263C8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C263D488> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C4826208> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000262C48B8408> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48BFD08> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48C2208> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48B5B88> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48CD1C8> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000262C48D5DC8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48CB5C8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48D90C8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48E0AC8> True\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x00000262C48CB688> True\n",
      "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x00000262C48E9E88> True\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune all the layers\n",
    "for layer in vgg_layers.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_layers.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 262144)            134479872 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 524290    \n",
      "=================================================================\n",
      "Total params: 155,028,546\n",
      "Trainable params: 155,028,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_layers)\n",
    "\n",
    "# add flatten layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# add dense layers with dropout\n",
    "model.add(tf.keras.layers.Dense(131072*2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.8))\n",
    "\n",
    "# add output layer\n",
    "#the labels I assigned to my training data is one-hot encoding, softmax is used\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# view model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "#Separate the labels from the image array for input into tensorflow\n",
    "train_labels=[i[1] for i in training_data]\n",
    "training=np.array([i[0] for i in training_data]).reshape(-1,size,size,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure that my data is in the correct format for tensorflow\n",
    "train_labels= tf.stack(train_labels)\n",
    "training = tf.stack(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "260/260 [==============================] - 34s 114ms/step - loss: 0.3959 - accuracy: 0.8062 - val_loss: 0.2305 - val_accuracy: 0.9100\n",
      "Epoch 2/100\n",
      "260/260 [==============================] - 28s 106ms/step - loss: 0.1878 - accuracy: 0.9272 - val_loss: 0.1837 - val_accuracy: 0.9284\n",
      "Epoch 3/100\n",
      "260/260 [==============================] - 28s 107ms/step - loss: 0.1516 - accuracy: 0.9402 - val_loss: 0.1552 - val_accuracy: 0.9328\n",
      "Epoch 4/100\n",
      "260/260 [==============================] - 28s 107ms/step - loss: 0.1283 - accuracy: 0.9508 - val_loss: 0.1452 - val_accuracy: 0.9425\n",
      "Epoch 5/100\n",
      "260/260 [==============================] - 28s 107ms/step - loss: 0.1065 - accuracy: 0.9595 - val_loss: 0.1688 - val_accuracy: 0.9371\n",
      "Epoch 6/100\n",
      "260/260 [==============================] - 28s 107ms/step - loss: 0.0884 - accuracy: 0.9667 - val_loss: 0.1473 - val_accuracy: 0.9425\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Early Stopping to prevent overfitting\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2,\n",
    "restore_best_weights=True,\n",
    "verbose=1)\n",
    "\n",
    "#Execution of the model training\n",
    "history = model.fit(training, train_labels,\n",
    "batch_size=32,\n",
    "callbacks=[es_callback],\n",
    "validation_split=0.1, epochs=EPOCHS,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function to prepare the testing data, uses the testing data csv to ensure that the data is\n",
    "# being read into prediction model in the correct order\n",
    "def test_data(path, excel):\n",
    "    excel=pd.read_csv(excel)\n",
    "    test = []\n",
    "    files = glob.glob(path+\"/*\") # get files in each folder(class)\n",
    "    for i in range(len(files)):\n",
    "        img = cv2.imread(path+'/'+excel['filename'].iloc[i]) #read the image\n",
    "        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE)) #resize the image\n",
    "        test.append([np.array(img)/255,excel['filename'].iloc[i]])\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare testing data\n",
    "testing_data= test_data('C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/test','C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/Testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[[0.43137255, 0.50980392, 0.67058824],\n",
       "          [0.43921569, 0.51372549, 0.67843137],\n",
       "          [0.44313725, 0.50980392, 0.67843137],\n",
       "          ...,\n",
       "          [0.49019608, 0.58431373, 0.86666667],\n",
       "          [0.49019608, 0.58431373, 0.86666667],\n",
       "          [0.50196078, 0.6       , 0.89019608]],\n",
       "  \n",
       "         [[0.42352941, 0.49803922, 0.66666667],\n",
       "          [0.41176471, 0.48627451, 0.65490196],\n",
       "          [0.43137255, 0.49803922, 0.67058824],\n",
       "          ...,\n",
       "          [0.47843137, 0.57254902, 0.85490196],\n",
       "          [0.47058824, 0.56470588, 0.84313725],\n",
       "          [0.4745098 , 0.57254902, 0.8627451 ]],\n",
       "  \n",
       "         [[0.44313725, 0.51764706, 0.69411765],\n",
       "          [0.41960784, 0.49411765, 0.67058824],\n",
       "          [0.40392157, 0.47843137, 0.65490196],\n",
       "          ...,\n",
       "          [0.47058824, 0.56470588, 0.83921569],\n",
       "          [0.45490196, 0.55294118, 0.82745098],\n",
       "          [0.46666667, 0.56470588, 0.84705882]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.42745098, 0.50588235, 0.75686275],\n",
       "          [0.44705882, 0.5254902 , 0.76470588],\n",
       "          [0.49411765, 0.56078431, 0.8       ],\n",
       "          ...,\n",
       "          [0.47843137, 0.56078431, 0.77254902],\n",
       "          [0.48627451, 0.56862745, 0.77254902],\n",
       "          [0.4627451 , 0.53333333, 0.74117647]],\n",
       "  \n",
       "         [[0.42352941, 0.50588235, 0.76078431],\n",
       "          [0.4627451 , 0.54117647, 0.79215686],\n",
       "          [0.49411765, 0.56078431, 0.80784314],\n",
       "          ...,\n",
       "          [0.44705882, 0.52156863, 0.74117647],\n",
       "          [0.4627451 , 0.53333333, 0.75294118],\n",
       "          [0.47058824, 0.53333333, 0.74901961]],\n",
       "  \n",
       "         [[0.41960784, 0.51372549, 0.76862745],\n",
       "          [0.47058824, 0.55294118, 0.80784314],\n",
       "          [0.49019608, 0.55686275, 0.80392157],\n",
       "          ...,\n",
       "          [0.4627451 , 0.52941176, 0.75294118],\n",
       "          [0.45098039, 0.51764706, 0.7372549 ],\n",
       "          [0.44705882, 0.50588235, 0.7254902 ]]]),\n",
       "  'Image_1.jpg'],\n",
       " [array([[[0.43137255, 0.6       , 0.82352941],\n",
       "          [0.46666667, 0.63529412, 0.85882353],\n",
       "          [0.48627451, 0.63921569, 0.86666667],\n",
       "          ...,\n",
       "          [0.21176471, 0.3372549 , 0.49803922],\n",
       "          [0.20392157, 0.32941176, 0.49019608],\n",
       "          [0.18039216, 0.30588235, 0.4745098 ]],\n",
       "  \n",
       "         [[0.38431373, 0.54117647, 0.77254902],\n",
       "          [0.42352941, 0.58039216, 0.80784314],\n",
       "          [0.45098039, 0.6       , 0.82745098],\n",
       "          ...,\n",
       "          [0.19215686, 0.30980392, 0.47058824],\n",
       "          [0.17254902, 0.29019608, 0.45098039],\n",
       "          [0.14901961, 0.2627451 , 0.43529412]],\n",
       "  \n",
       "         [[0.37254902, 0.51372549, 0.75686275],\n",
       "          [0.40784314, 0.54901961, 0.78431373],\n",
       "          [0.43137255, 0.56862745, 0.79607843],\n",
       "          ...,\n",
       "          [0.15294118, 0.2627451 , 0.42352941],\n",
       "          [0.13333333, 0.24705882, 0.40784314],\n",
       "          [0.1254902 , 0.23921569, 0.4       ]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.28235294, 0.35294118, 0.70980392],\n",
       "          [0.28627451, 0.35686275, 0.71372549],\n",
       "          [0.29411765, 0.36862745, 0.7254902 ],\n",
       "          ...,\n",
       "          [0.32156863, 0.43137255, 0.77254902],\n",
       "          [0.32941176, 0.43921569, 0.78039216],\n",
       "          [0.31372549, 0.43137255, 0.77254902]],\n",
       "  \n",
       "         [[0.27058824, 0.3372549 , 0.70196078],\n",
       "          [0.28627451, 0.35686275, 0.71372549],\n",
       "          [0.30588235, 0.37647059, 0.74117647],\n",
       "          ...,\n",
       "          [0.29019608, 0.4       , 0.74117647],\n",
       "          [0.27843137, 0.38823529, 0.72941176],\n",
       "          [0.25490196, 0.37254902, 0.71372549]],\n",
       "  \n",
       "         [[0.25882353, 0.32941176, 0.70196078],\n",
       "          [0.27843137, 0.34901961, 0.71764706],\n",
       "          [0.31372549, 0.38431373, 0.75294118],\n",
       "          ...,\n",
       "          [0.32156863, 0.43137255, 0.77254902],\n",
       "          [0.29803922, 0.40784314, 0.74901961],\n",
       "          [0.27058824, 0.38823529, 0.72941176]]]),\n",
       "  'Image_2.jpg'],\n",
       " [array([[[0.64313725, 0.76078431, 0.98431373],\n",
       "          [0.62352941, 0.74509804, 0.96470588],\n",
       "          [0.61176471, 0.72941176, 0.95294118],\n",
       "          ...,\n",
       "          [0.50980392, 0.64705882, 0.86666667],\n",
       "          [0.53333333, 0.67058824, 0.89019608],\n",
       "          [0.54509804, 0.68235294, 0.90196078]],\n",
       "  \n",
       "         [[0.63529412, 0.75294118, 0.97647059],\n",
       "          [0.62745098, 0.74509804, 0.96862745],\n",
       "          [0.61568627, 0.73333333, 0.95686275],\n",
       "          ...,\n",
       "          [0.49019608, 0.63529412, 0.85490196],\n",
       "          [0.50980392, 0.64313725, 0.8745098 ],\n",
       "          [0.51372549, 0.65882353, 0.87843137]],\n",
       "  \n",
       "         [[0.63529412, 0.74509804, 0.98039216],\n",
       "          [0.63137255, 0.74117647, 0.97647059],\n",
       "          [0.62352941, 0.74117647, 0.96862745],\n",
       "          ...,\n",
       "          [0.49019608, 0.63529412, 0.8627451 ],\n",
       "          [0.50196078, 0.64313725, 0.87843137],\n",
       "          [0.51372549, 0.6627451 , 0.88627451]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.69803922, 0.78823529, 0.98431373],\n",
       "          [0.70980392, 0.8       , 0.99607843],\n",
       "          [0.71372549, 0.80392157, 1.        ],\n",
       "          ...,\n",
       "          [0.7254902 , 0.82352941, 0.99607843],\n",
       "          [0.73333333, 0.83137255, 1.        ],\n",
       "          [0.73333333, 0.83137255, 1.        ]],\n",
       "  \n",
       "         [[0.69019608, 0.78039216, 0.97647059],\n",
       "          [0.70196078, 0.79215686, 0.98823529],\n",
       "          [0.70980392, 0.80392157, 0.99607843],\n",
       "          ...,\n",
       "          [0.72941176, 0.82745098, 1.        ],\n",
       "          [0.7372549 , 0.83529412, 1.        ],\n",
       "          [0.7372549 , 0.83529412, 1.        ]],\n",
       "  \n",
       "         [[0.68627451, 0.77647059, 0.97254902],\n",
       "          [0.70196078, 0.79215686, 0.98823529],\n",
       "          [0.70980392, 0.80392157, 1.        ],\n",
       "          ...,\n",
       "          [0.72156863, 0.81960784, 0.99215686],\n",
       "          [0.72941176, 0.82745098, 1.        ],\n",
       "          [0.72941176, 0.82745098, 1.        ]]]),\n",
       "  'Image_3.jpg'],\n",
       " [array([[[0.13333333, 0.17647059, 0.16862745],\n",
       "          [0.11764706, 0.16078431, 0.15294118],\n",
       "          [0.10980392, 0.14901961, 0.14901961],\n",
       "          ...,\n",
       "          [0.56078431, 0.65490196, 0.74117647],\n",
       "          [0.60392157, 0.69411765, 0.79607843],\n",
       "          [0.58431373, 0.6745098 , 0.77647059]],\n",
       "  \n",
       "         [[0.04705882, 0.08627451, 0.07843137],\n",
       "          [0.04705882, 0.08627451, 0.08235294],\n",
       "          [0.0627451 , 0.10196078, 0.10196078],\n",
       "          ...,\n",
       "          [0.52941176, 0.62745098, 0.71372549],\n",
       "          [0.57254902, 0.67058824, 0.76470588],\n",
       "          [0.57254902, 0.67058824, 0.77254902]],\n",
       "  \n",
       "         [[0.04705882, 0.07843137, 0.0745098 ],\n",
       "          [0.04705882, 0.0745098 , 0.0745098 ],\n",
       "          [0.05882353, 0.09019608, 0.09019608],\n",
       "          ...,\n",
       "          [0.50980392, 0.61176471, 0.69803922],\n",
       "          [0.51372549, 0.61568627, 0.70980392],\n",
       "          [0.52156863, 0.62745098, 0.7254902 ]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.44705882, 0.56470588, 0.63921569],\n",
       "          [0.48627451, 0.6       , 0.68235294],\n",
       "          [0.50588235, 0.62352941, 0.71764706],\n",
       "          ...,\n",
       "          [0.54901961, 0.6627451 , 0.80784314],\n",
       "          [0.53333333, 0.64705882, 0.8       ],\n",
       "          [0.5254902 , 0.63921569, 0.79215686]],\n",
       "  \n",
       "         [[0.44705882, 0.56078431, 0.64313725],\n",
       "          [0.48627451, 0.60392157, 0.68627451],\n",
       "          [0.50588235, 0.62745098, 0.71764706],\n",
       "          ...,\n",
       "          [0.54509804, 0.65882353, 0.80784314],\n",
       "          [0.53333333, 0.64705882, 0.80392157],\n",
       "          [0.52156863, 0.63137255, 0.79215686]],\n",
       "  \n",
       "         [[0.44705882, 0.56078431, 0.64313725],\n",
       "          [0.49019608, 0.60392157, 0.68627451],\n",
       "          [0.50980392, 0.63137255, 0.72156863],\n",
       "          ...,\n",
       "          [0.54117647, 0.65882353, 0.80784314],\n",
       "          [0.53333333, 0.64313725, 0.80392157],\n",
       "          [0.52156863, 0.63137255, 0.79215686]]]),\n",
       "  'Image_4.jpg'],\n",
       " [array([[[0.62745098, 0.76470588, 0.96078431],\n",
       "          [0.63921569, 0.77254902, 0.96862745],\n",
       "          [0.65882353, 0.78431373, 0.97647059],\n",
       "          ...,\n",
       "          [0.38823529, 0.4745098 , 0.72941176],\n",
       "          [0.38039216, 0.46666667, 0.72156863],\n",
       "          [0.38039216, 0.46666667, 0.72156863]],\n",
       "  \n",
       "         [[0.63921569, 0.77647059, 0.97254902],\n",
       "          [0.65882353, 0.78823529, 0.98039216],\n",
       "          [0.67058824, 0.79607843, 0.98823529],\n",
       "          ...,\n",
       "          [0.38039216, 0.46666667, 0.71764706],\n",
       "          [0.36862745, 0.45490196, 0.70980392],\n",
       "          [0.36470588, 0.45098039, 0.70196078]],\n",
       "  \n",
       "         [[0.65882353, 0.78823529, 0.98039216],\n",
       "          [0.66666667, 0.79607843, 0.98823529],\n",
       "          [0.67058824, 0.79607843, 0.98431373],\n",
       "          ...,\n",
       "          [0.36078431, 0.45098039, 0.69803922],\n",
       "          [0.34509804, 0.43529412, 0.67843137],\n",
       "          [0.34117647, 0.43137255, 0.6745098 ]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.6       , 0.72941176, 0.97647059],\n",
       "          [0.59607843, 0.7254902 , 0.97254902],\n",
       "          [0.59607843, 0.7254902 , 0.96862745],\n",
       "          ...,\n",
       "          [0.65098039, 0.74509804, 0.95686275],\n",
       "          [0.65490196, 0.74901961, 0.96078431],\n",
       "          [0.6627451 , 0.75686275, 0.96078431]],\n",
       "  \n",
       "         [[0.59215686, 0.72156863, 0.96862745],\n",
       "          [0.58823529, 0.71764706, 0.96470588],\n",
       "          [0.58823529, 0.71764706, 0.96470588],\n",
       "          ...,\n",
       "          [0.65882353, 0.75294118, 0.96470588],\n",
       "          [0.6627451 , 0.75686275, 0.96862745],\n",
       "          [0.67058824, 0.76470588, 0.96862745]],\n",
       "  \n",
       "         [[0.58823529, 0.71764706, 0.96470588],\n",
       "          [0.58431373, 0.71372549, 0.96078431],\n",
       "          [0.58431373, 0.71372549, 0.96078431],\n",
       "          ...,\n",
       "          [0.65882353, 0.74117647, 0.95686275],\n",
       "          [0.6627451 , 0.74509804, 0.96078431],\n",
       "          [0.65490196, 0.74901961, 0.95294118]]]),\n",
       "  'Image_5.jpg']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the testing data is being read in correctly\n",
    "testing_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the test image into the correct format for the model to read\n",
    "test_x = np.array([i[0] for i in testing_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "test_x=tf.stack(test_x)\n",
    "prediction=model.predict(test_x) #predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99985337e-01, 1.46421180e-05],\n",
       "       [7.06477940e-01, 2.93522060e-01],\n",
       "       [8.53563070e-01, 1.46436870e-01],\n",
       "       [9.94184673e-01, 5.81538444e-03],\n",
       "       [9.71890330e-01, 2.81096939e-02],\n",
       "       [9.88357961e-01, 1.16420379e-02],\n",
       "       [1.90202962e-03, 9.98097956e-01],\n",
       "       [9.81286943e-01, 1.87130719e-02],\n",
       "       [9.46390331e-01, 5.36097065e-02],\n",
       "       [1.13956441e-04, 9.99886036e-01],\n",
       "       [5.16945720e-01, 4.83054280e-01],\n",
       "       [1.16520934e-01, 8.83479059e-01],\n",
       "       [8.91323611e-02, 9.10867631e-01],\n",
       "       [9.94980991e-01, 5.01901843e-03],\n",
       "       [9.84669685e-01, 1.53302625e-02]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observe the first fifteen predictions\n",
    "prediction[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the predicted probability of each category to its category based on the highest predicted probability\n",
    "test_result= np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#observe the test result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for export and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataframe for manipulation for export\n",
    "result=pd.DataFrame(data=test_result, index=None, columns=[\"label\"]) #create dataframe for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2305 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0       male\n",
       "1       male\n",
       "2       male\n",
       "3       male\n",
       "4       male\n",
       "...      ...\n",
       "2300  female\n",
       "2301  female\n",
       "2302    male\n",
       "2303    male\n",
       "2304    male\n",
       "\n",
       "[2305 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert numerical output of the model back their categorical name\n",
    "new_value = {0:'male', 1:'female'}\n",
    "result['label'].replace(new_value,inplace=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data as csv\n",
    "result[\"label\"].to_csv('C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/submission_3.csv', mode = 'w' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('C:/Users/weeho/Desktop/Deep Learning Bootcamp/Datathon/eye_gender_data/my_model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
